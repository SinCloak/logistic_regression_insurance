逻辑回归（Logistic Regression）是一种经典的监督学习算法，广泛用于分类任务，尤其是二分类问题。尽管名字中带有“回归”，但逻辑回归的主要目标是预测类别，而不是连续值。

### 1. **核心思想**
逻辑回归的核心思想是通过线性模型（类似于线性回归）计算一个线性组合，将其结果输入到一个逻辑函数（sigmoid函数）中，输出一个范围在0到1之间的概率值。这种概率可以用来做二分类任务，比如预测样本属于某个类别的概率。

### 2. **数学形式**

逻辑回归的核心方程分为两个部分：
- **线性部分**：线性回归模型中的部分
  \[
  z = w_1x_1 + w_2x_2 + \cdots + w_nx_n + b
  \]
  其中，\(w_1, w_2, ..., w_n\) 是权重，\(x_1, x_2, ..., x_n\) 是输入特征，\(b\) 是偏置项。

- **非线性部分**：通过Sigmoid函数将线性输出转化为概率
  \[
  p(y=1|x) = \frac{1}{1 + e^{-z}} = \sigma(z)
  \]
  其中，\(\sigma(z)\) 是 Sigmoid 函数，表示某样本属于类别 1 的概率。

### 3. **Sigmoid函数**

Sigmoid函数是一个S形函数，它将线性模型的输出值转化为0到1之间的值，便于解释为概率。其数学表达式为：
\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]

- 当 \(z \to \infty\) 时， \(\sigma(z) \to 1\)。
- 当 \(z \to -\infty\) 时， \(\sigma(z) \to 0\)。

这使得逻辑回归的输出可以自然地解释为类别的概率。

### 4. **决策边界**
逻辑回归通过概率阈值来确定分类。如果预测的概率大于0.5，则预测为类别1，否则为类别0。也可以根据任务需要调整阈值，比如设定为0.3或0.7。

### 5. **模型训练**

#### 5.1 **损失函数**
为了训练逻辑回归模型，我们需要一个损失函数来衡量模型预测与真实标签之间的差距。逻辑回归使用**对数损失函数**（log loss），又称为交叉熵损失：
\[
L(y, \hat{y}) = -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right)
\]
其中，\(\hat{y}^{(i)}\) 是第 \(i\) 个样本的预测值，\(y^{(i)}\) 是第 \(i\) 个样本的真实标签，\(m\) 是样本总数。

#### 5.2 **梯度下降**
逻辑回归通过梯度下降法（或变种如批量梯度下降、随机梯度下降）来最小化损失函数，并更新模型的权重和偏置项。

梯度下降的步骤为：
\[
w_j \leftarrow w_j - \alpha \frac{\partial L}{\partial w_j}
\]
其中，\(\alpha\) 是学习率，\(\frac{\partial L}{\partial w_j}\) 是相对于权重 \(w_j\) 的损失函数的偏导数。

### 6. **扩展到多分类**
逻辑回归不仅可以用于二分类问题，还可以通过以下两种方式扩展到多分类问题：
1. **一对多（OvR）策略**：将多分类问题拆解为多个二分类问题，例如针对每个类别，建立一个二分类模型，预测样本是否属于该类别。
2. **Softmax回归**：用于多分类的直接推广，将 sigmoid 函数替换为 softmax 函数，输出每个类别的概率，类别数大于 2 时适用。

### 7. **正则化**
为了防止过拟合，逻辑回归通常加入正则化项。常用的正则化方法包括：
- **L1正则化（Lasso）**：在损失函数中加入权重绝对值的惩罚项，鼓励稀疏解，较小的权重会变成0，方便特征选择。
- **L2正则化（Ridge）**：在损失函数中加入权重平方的惩罚项，防止过拟合。

正则化后的损失函数为：
\[
L = -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right) + \frac{\lambda}{2} \sum_{j=1}^{n} w_j^2
\]
其中，\(\lambda\) 是正则化强度的控制参数。

### 8. **优点与缺点**

#### 8.1 优点：
- **可解释性强**：逻辑回归的输出是概率，可以轻松解释模型的预测。
- **训练效率高**：由于其模型相对简单，训练速度较快，适合大规模数据集。
- **对线性可分数据表现较好**。

#### 8.2 缺点：
- **对线性关系的依赖性**：逻辑回归假设特征和目标变量之间是线性关系，对于非线性可分问题表现较差。
- **难以处理高度相关的特征**：如果特征之间高度相关，模型的表现会受到影响，需要引入正则化。
- **难以处理类别严重不平衡**：逻辑回归在类别不平衡的情况下表现不佳。

### 9. **应用场景**
- **医学领域**：预测患者是否患有某种疾病。
- **市场营销**：预测客户是否会购买某个产品。
- **金融领域**：预测客户是否会违约、欺诈检测等。

逻辑回归虽然简单，但由于其解释性强、易于实现，仍然是很多分类问题中的首选模型。
