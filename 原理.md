Markdown 中确实不支持 LaTeX 公式的直接显示。你可以尝试将公式文本转换为图片或使用支持 LaTeX 的平台（如 Jupyter Notebook 或一些 Markdown 编辑器）。另外，也可以通过调整公式的符号或换用文字解释来适应不同的格式。以下是逻辑回归内容的调整版，适合一般的 Markdown 格式。

---

## 逻辑回归（Logistic Regression）

逻辑回归是一种用于**分类任务**的监督学习算法，主要解决**二分类问题**。它将输入特征的线性组合通过**Sigmoid函数**映射到 [0, 1] 的区间，作为类别的概率输出。

---

### 1. 核心思想

逻辑回归首先计算输入特征的线性组合：

```
z = w1 * x1 + w2 * x2 + ... + wn * xn + b
```

其中：
- **wi** 是每个特征的权重
- **xi** 是对应的特征值
- **b** 是偏置项

然后，通过 **Sigmoid 函数**将 `z` 转换为概率：

```
p(y=1 | x) = 1 / (1 + exp(-z))
```

---

### 2. Sigmoid 函数

Sigmoid 是一个 **S 形函数**，将线性输出值映射到 [0, 1] 区间：

```
σ(z) = 1 / (1 + exp(-z))
```

- 当 z 趋向无穷大时，σ(z) → 1  
- 当 z 趋向负无穷大时，σ(z) → 0

Sigmoid 函数的输出可被解释为预测样本属于类别 1 的概率。

---

### 3. 决策边界

逻辑回归通常使用 0.5 作为阈值：
- 如果概率 ≥ 0.5，则预测为类别 1
- 如果概率 < 0.5，则预测为类别 0

根据需要，可以调整这个阈值来适应不同的业务场景。

---

### 4. 模型训练

逻辑回归使用 **交叉熵损失函数**来衡量预测结果的准确性：

```
Loss = -(1/m) * Σ [y * log(ŷ) + (1 - y) * log(1 - ŷ)]
```

其中：
- **m** 是样本总数
- **y** 是真实标签
- **ŷ** 是预测的概率

#### 梯度下降

通过 **梯度下降法**不断更新权重，最小化损失函数：

```
wj = wj - α * ∂(Loss) / ∂(wj)
```

- **wj** 是第 j 个特征的权重
- **α** 是学习率

---

### 5. 正则化

为防止模型过拟合，可以在损失函数中加入正则化项。

- **L1 正则化（Lasso）**：鼓励稀疏性，使部分权重为 0，有助于特征选择。
- **L2 正则化（Ridge）**：减少权重的平方和，防止模型过拟合。

正则化后的损失函数如下：

```
Loss = -(1/m) * Σ [y * log(ŷ) + (1 - y) * log(1 - ŷ)] + (λ/2) * Σ (wj^2)
```

其中，**λ** 是正则化强度的超参数。

---

### 6. 扩展到多分类问题

逻辑回归可以通过以下两种方式扩展为多分类：
1. **一对多策略（OvR）**：对每个类别构建二分类模型，预测是否属于该类别。
2. **Softmax 回归**：将 Sigmoid 替换为 Softmax，适用于多类别分类。

---

### 7. 优缺点

#### 优点
- **可解释性强**：输出概率容易理解。
- **计算效率高**：训练速度快，适合大数据集。
- **表现稳定**：在线性可分问题上效果较好。

#### 缺点
- **依赖线性关系**：对于非线性问题效果不佳。
- **难以应对多重共线性**：需要正则化来处理。
- **对类别不平衡敏感**：需要调整阈值或采样策略。

---

### 8. 应用场景

- **金融**：信用评分、违约预测
- **医学**：疾病诊断预测
- **市场营销**：客户行为预测
- **互联网**：垃圾邮件检测

---

这就是逻辑回归的核心原理和应用场景。它因简单易用且易于解释，依然是众多分类问题的常用工具。

---

如果需要更复杂的公式呈现，建议在 Jupyter Notebook、VSCode 或 Google Colab 等支持 LaTeX 的环境中查看内容。
